import{_ as o,r as a,o as s,c as d,a as e,b as t,d as r,e as c}from"./app.76e8dadb.js";const i={},p=c('<h1 id="train-and-inference-speed-of-cuda-and-mac-with-mps" tabindex="-1"><a class="header-anchor" href="#train-and-inference-speed-of-cuda-and-mac-with-mps" aria-hidden="true">#</a> Train And Inference Speed of CUDA and Mac with MPS</h1><blockquote><p>First written\uFF1A2023/06/17</p></blockquote><p>As some deep learning frameworks support <strong>MPS backend</strong> on the ARM Mac, the GPU of the Mac can now be used for deep learning tasks.</p><p>There are many specific configuration methods on the network, and they are very simple, so I will not repeat them here.</p><p>I mainly compare the speed differences between the <strong>MPS</strong> backend and the <strong>CUDA</strong> backend.</p><table><thead><tr><th></th><th>Train</th><th>Inference</th></tr></thead><tbody><tr><td>Nvidia Geforce RTX3060 12GB (Windows, CUDA)</td><td>4.66x</td><td>1.43x</td></tr><tr><td><strong>Nvidia Tesla P100 (Kaggle, CUDA)</strong></td><td><strong>6.03x</strong></td><td><strong>1.45x</strong></td></tr><tr><td>Apple M1 GPU (7-core, MPS)</td><td>1.00x</td><td>1.00x</td></tr></tbody></table>',6),h=e("p",null,[e("strong",null,"Baseline"),t(" (All test are based on PyTorch)")],-1),l={href:"https://huggingface.co/docs/transformers/quicktour#trainer-a-pytorch-optimized-training-loop",target:"_blank",rel:"noopener noreferrer"},_={href:"https://huggingface.co/docs/transformers/quicktour#pipeline",target:"_blank",rel:"noopener noreferrer"};function f(g,u){const n=a("ExternalLinkIcon");return s(),d("div",null,[p,e("blockquote",null,[h,e("p",null,[t("Train Speed: "),e("a",l,[t("https://huggingface.co/docs/transformers/quicktour#trainer-a-pytorch-optimized-training-loop"),r(n)])]),e("p",null,[t("Inference Speed: "),e("a",_,[t("https://huggingface.co/docs/transformers/quicktour#pipeline"),r(n)])])])])}const k=o(i,[["render",f],["__file","train_and_inference_speed_of_cuda_and_mps_on_mac.html.vue"]]);export{k as default};
